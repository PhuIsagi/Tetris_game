hÆ°Æ¡ng 3. PhÆ°Æ¡ng phÃ¡p Ä‘á» xuáº¥t
3.1. Tá»•ng quan chiáº¿n lÆ°á»£c AI
3.1.1. Má»¥c tiÃªu xÃ¢y dá»±ng AI cho Tetris

Tetris lÃ  má»™t trÃ² chÆ¡i cÃ³ luáº­t Ä‘Æ¡n giáº£n nhÆ°ng khÃ´ng gian tráº¡ng thÃ¡i ráº¥t lá»›n. Má»¥c tiÃªu khi xÃ¢y dá»±ng AI gá»“m:

Ra quyáº¿t Ä‘á»‹nh tá»‘i Æ°u trong thá»i gian thá»±c: Chá»n hÆ°á»›ng xoay vÃ  vá»‹ trÃ­ tháº£ khá»‘i nhanh chÃ³ng.

Duy trÃ¬ tráº¡ng thÃ¡i bá»n vá»¯ng: Háº¡n cháº¿ táº¡o ra cá»™t quÃ¡ cao, trÃ¡nh game over sá»›m.

Tá»‘i Ä‘a hÃ³a Ä‘iá»ƒm sá»‘: XÃ³a cÃ ng nhiá»u dÃ²ng cÃ ng tá»‘t, Æ°u tiÃªn combo.

Kháº£ nÄƒng tá»•ng quÃ¡t: MÃ´ hÃ¬nh cÃ³ thá»ƒ Ã¡p dá»¥ng cho cÃ¡c biáº¿n thá»ƒ cá»§a Tetris vá»›i kÃ­ch thÆ°á»›c báº£ng hoáº·c tá»‘c Ä‘á»™ rÆ¡i khÃ¡c nhau.

3.1.2. TiÃªu chÃ­ Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£

CÃ¡c tiÃªu chÃ­ chÃ­nh:

Tá»•ng Ä‘iá»ƒm trung bÃ¬nh (Average score): pháº£n Ã¡nh hiá»‡u quáº£ tá»•ng quÃ¡t.

Sá»‘ dÃ²ng xÃ³a (Lines cleared): Ä‘o lÆ°á»ng kháº£ nÄƒng duy trÃ¬ báº£ng chÆ¡i.

Chiá»u cao cá»±c Ä‘áº¡i (Maximum height): xÃ¡c Ä‘á»‹nh má»©c Ä‘á»™ nguy hiá»ƒm cá»§a board.

Thá»i gian sá»‘ng sÃ³t (Survival time): sá»‘ bÆ°á»›c Ä‘i hoáº·c khá»‘i xá»­ lÃ½ Ä‘Æ°á»£c.

Hiá»‡u suáº¥t tÃ­nh toÃ¡n (Computational efficiency): thá»i gian dá»± Ä‘oÃ¡n cho má»—i khá»‘i.

3.1.3. So sÃ¡nh vÃ  Ä‘á»‹nh hÆ°á»›ng giá»¯a cÃ¡c phÆ°Æ¡ng phÃ¡p

Ba hÆ°á»›ng tiáº¿p cáº­n chÃ­nh:

PhÆ°Æ¡ng phÃ¡p	CÆ¡ cháº¿	Æ¯u Ä‘iá»ƒm	Háº¡n cháº¿	á»¨ng dá»¥ng Ä‘iá»ƒn hÃ¬nh
Heuristic-based	HÃ m Ä‘Ã¡nh giÃ¡ thá»§ cÃ´ng dá»±a trÃªn cÃ¡c Ä‘áº·c trÆ°ng (sá»‘ lá»—, chiá»u cao, gá»“ ghá»â€¦)	ÄÆ¡n giáº£n, nhanh, dá»… triá»ƒn khai	KhÃ³ má»Ÿ rá»™ng, phá»¥ thuá»™c con ngÆ°á»i	Dellacherie (2003)
Reinforcement Learning	Há»c tá»« pháº§n thÆ°á»Ÿng trong MDP	Tá»± há»c chiáº¿n lÆ°á»£c, khÃ¡i quÃ¡t tá»‘t	Huáº¥n luyá»‡n lÃ¢u, dá»… máº¥t á»•n Ä‘á»‹nh	Q-learning, DQN, PPO
Deep Learning	Máº¡ng nÆ¡-ron há»c biá»ƒu diá»…n tráº¡ng thÃ¡i	Há»c Ä‘áº·c trÆ°ng phá»©c táº¡p, káº¿t há»£p RL	Tá»‘n tÃ i nguyÃªn, khÃ³ giáº£i thÃ­ch	DQN, Actor-Critic Deep

Äá»‹nh hÆ°á»›ng Ä‘á» xuáº¥t:

Sá»­ dá»¥ng heuristic lÃ m baseline Ä‘á»ƒ Ä‘áº£m báº£o tá»‘c Ä‘á»™ vÃ  á»•n Ä‘á»‹nh.

TÃ­ch há»£p RL Ä‘á»ƒ tá»‘i Æ°u chiáº¿n lÆ°á»£c dÃ i háº¡n.

Sá»­ dá»¥ng DL Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»± Ä‘á»™ng vÃ  khÃ¡i quÃ¡t hÃ³a.

3.2. Heuristic-based AI
3.2.1. NguyÃªn lÃ½ hoáº¡t Ä‘á»™ng

Má»—i tráº¡ng thÃ¡i board Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ báº±ng hÃ m Score(s):

ğ‘†
ğ‘
ğ‘œ
ğ‘Ÿ
ğ‘’
(
ğ‘ 
)
=
âˆ‘
ğ‘–
ğ‘¤
ğ‘–
â‹…
ğ‘“
ğ‘–
(
ğ‘ 
)
Score(s)=
i
âˆ‘
	â€‹

w
i
	â€‹

â‹…f
i
	â€‹

(s)

Trong Ä‘Ã³:

ğ‘“
ğ‘–
(
ğ‘ 
)
f
i
	â€‹

(s) lÃ  Ä‘áº·c trÆ°ng cá»§a board (sá»‘ dÃ²ng xÃ³a, sá»‘ lá»—, chiá»u cao, bumpiness).

ğ‘¤
ğ‘–
w
i
	â€‹

 lÃ  trá»ng sá»‘ Ä‘Æ°á»£c tinh chá»‰nh thá»§ cÃ´ng.

AI chá»n hÃ nh Ä‘á»™ng táº¡o ra tráº¡ng thÃ¡i cÃ³ Ä‘iá»ƒm sá»‘ cao nháº¥t.

3.2.2. HÃ m Ä‘Ã¡nh giÃ¡ vÃ  cÃ¡c chá»‰ sá»‘

CÃ¡c Ä‘áº·c trÆ°ng chÃ­nh:

Aggregate height: tá»•ng chiá»u cao cÃ¡c cá»™t.

Number of holes: sá»‘ Ã´ trá»‘ng bá»‹ che phÃ­a trÃªn.

Bumpiness: chÃªnh lá»‡ch chiá»u cao giá»¯a cÃ¡c cá»™t.

Completed lines: sá»‘ dÃ²ng xÃ³a ngay sau khi Ä‘áº·t khá»‘i.

VÃ­ dá»¥ hÃ m Ä‘Ã¡nh giÃ¡ Dellacherie (2003):

ğ‘‰
(
ğ‘ 
)
=
ğ‘¤
1
â‹…
ğ¿
ğ‘–
ğ‘›
ğ‘’
ğ‘ 
âˆ’
ğ‘¤
2
â‹…
ğ»
ğ‘œ
ğ‘™
ğ‘’
ğ‘ 
âˆ’
ğ‘¤
3
â‹…
ğ»
ğ‘’
ğ‘–
ğ‘”
â„
ğ‘¡
âˆ’
ğ‘¤
4
â‹…
ğµ
ğ‘¢
ğ‘š
ğ‘
ğ‘–
ğ‘›
ğ‘’
ğ‘ 
ğ‘ 
V(s)=w
1
	â€‹

â‹…Linesâˆ’w
2
	â€‹

â‹…Holesâˆ’w
3
	â€‹

â‹…Heightâˆ’w
4
	â€‹

â‹…Bumpiness
3.2.3. Æ¯u Ä‘iá»ƒm vÃ  háº¡n cháº¿

Æ¯u Ä‘iá»ƒm: Ä‘Æ¡n giáº£n, nhanh, khÃ´ng cáº§n huáº¥n luyá»‡n.

Háº¡n cháº¿: phá»¥ thuá»™c thiáº¿t káº¿ trá»ng sá»‘, khÃ´ng thÃ­ch á»©ng khi luáº­t thay Ä‘á»•i, thÆ°á»ng khÃ´ng Ä‘áº¡t Ä‘iá»ƒm tá»‘i Ä‘a nhÆ° RL.

3.3. Reinforcement Learning AI
3.3.1. MÃ´ hÃ¬nh MDP cho Tetris

State (S): ma tráº­n board + khá»‘i hiá»‡n táº¡i.

Action (A): hÆ°á»›ng xoay + vá»‹ trÃ­ cá»™t.

Reward (R): Ä‘iá»ƒm xÃ³a dÃ²ng.

Transition (T): quy táº¯c váº­t lÃ½ game.

Objective: tá»‘i Ä‘a hÃ³a pháº§n thÆ°á»Ÿng tÃ­ch lÅ©y.

3.3.2. Thuáº­t toÃ¡n há»c

Q-learning: báº£ng Q cho khÃ´ng gian nhá».

Deep Q-Network (DQN): sá»­ dá»¥ng CNN Ä‘á»ƒ xáº¥p xá»‰ Q-function cho board lá»›n.

3.3.3. Kiáº¿n trÃºc máº¡ng DQNCNN

Conv layers: 2 lá»›p, 32 vÃ  64 filter, kernel 3Ã—3, ReLU.

Flatten â†’ Fully connected: dá»± Ä‘oÃ¡n giÃ¡ trá»‹ Q cho rotation vÃ  x-position (2 nhÃ¡nh).

Replay buffer: lÆ°u tráº£i nghiá»‡m, minibatch há»c ngáº«u nhiÃªn.

ChÃ­nh sÃ¡ch Îµ-greedy: cÃ¢n báº±ng giá»¯a khÃ¡m phÃ¡ vÃ  khai thÃ¡c.

3.3.4. Quy trÃ¬nh huáº¥n luyá»‡n

Reset board.

Agent chá»n hÃ nh Ä‘á»™ng theo Îµ-greedy.

Thá»±c thi action, nháº­n next_state vÃ  reward.

LÆ°u tráº£i nghiá»‡m vÃ  huáº¥n luyá»‡n báº±ng minibatch.

Cáº­p nháº­t Q-network báº±ng cÃ´ng thá»©c Bellman:

ğ‘„
ğ‘¡
ğ‘
ğ‘Ÿ
ğ‘”
ğ‘’
ğ‘¡
=
ğ‘Ÿ
+
ğ›¾
max
â¡
ğ‘„
(
ğ‘ 
â€²
,
ğ‘
â€²
)
Q
target
	â€‹

=r+Î³maxQ(s
â€²
,a
â€²
)

Láº·p láº¡i nhiá»u episode, lÆ°u mÃ´ hÃ¬nh tá»‘t nháº¥t.

3.3.5. Æ¯u Ä‘iá»ƒm vÃ  háº¡n cháº¿

Æ¯u Ä‘iá»ƒm: há»c chiáº¿n lÆ°á»£c tá»‘i Æ°u, thÃ­ch á»©ng.

Háº¡n cháº¿: cáº§n thá»i gian huáº¥n luyá»‡n, dá»… dao Ä‘á»™ng, tá»‘n tÃ i nguyÃªn.

3.4. Deep Learning AI
3.4.1. Kiáº¿n trÃºc máº¡ng

Input: ma tráº­n board 15Ã—25 (flatten hoáº·c kÃªnh CNN).

Network: Fully-connected 3 lá»›p (256-128-64), ReLU.

Output: action space rotation + position.

3.4.2. QuÃ¡ trÃ¬nh huáº¥n luyá»‡n

Dataset: cáº·p (state, best action tá»« heuristic hoáº·c RL).

Loss: Cross-entropy (supervised) hoáº·c MSE (DQN).

Optimizer: Adam, learning rate = 0.001.

3.4.3. Æ¯u Ä‘iá»ƒm vÃ  háº¡n cháº¿

Æ¯u Ä‘iá»ƒm: há»c Ä‘áº·c trÆ°ng phá»©c táº¡p, dá»± Ä‘oÃ¡n nhanh, khÃ¡i quÃ¡t tá»‘t.

Háº¡n cháº¿: phá»¥ thuá»™c cháº¥t lÆ°á»£ng dataset, chi phÃ­ tÃ­nh toÃ¡n cao, khÃ³ giáº£i thÃ­ch quyáº¿t Ä‘á»‹nh.

ChÆ°Æ¡ng 4. Káº¿t quáº£ vÃ  thá»±c nghiá»‡m
4.1. Thiáº¿t láº­p thá»±c nghiá»‡m
4.1.1. MÃ´i trÆ°á»ng vÃ  tham sá»‘

NgÃ´n ngá»¯: Python 3.13

ThÆ° viá»‡n: PyQt5 (GUI), NumPy, PyTorch

Cáº¥u hÃ¬nh: Intel Core i5-1240P, RAM 8GB, Windows 11

KÃ­ch thÆ°á»›c board: 15Ã—25

Sá»‘ láº§n cháº¡y thá»­ nghiá»‡m: 10 láº§n cho má»—i thiáº¿t láº­p, tá»•ng 100 lÆ°á»£t.

4.1.2. Dá»¯ liá»‡u vÃ  dataset

Heuristic AI: (board state, move), Ä‘iá»ƒm sá»‘, sá»‘ dÃ²ng xÃ³a.

RL AI: (state, action, reward) theo episode.

DL AI: (state, best action) tá»« heuristic hoáº·c RL (teacher forcing).

LÆ°u trá»¯: dÃ¹ng joblib (.pkl) Ä‘á»ƒ tá»‘i Æ°u tá»‘c Ä‘á»™ Ä‘á»c/ghi.

4.1.3. CÃ¡c phÆ°Æ¡ng phÃ¡p triá»ƒn khai

Heuristic: score function Dellacherie má»Ÿ rá»™ng.

RL: DQN agent vá»›i Îµ-greedy, replay buffer.

DL: máº¡ng fully-connected 3 lá»›p, input flatten 375 chiá»u, output rotation + position.

4.2. Dataset
PhÆ°Æ¡ng phÃ¡p	Sá»‘ máº«u	Äáº·c trÆ°ng chÃ­nh	Ghi chÃº
Heuristic	10.000	board, move, score, lines cleared	Trung bÃ¬nh má»—i vÃ¡n 200â€“300 tráº¡ng thÃ¡i
RL	50.000	state, action, reward	Reward trung bÃ¬nh tÄƒng tá»« 50 â†’ 2.500 sau 200 episodes
DL	15.000	state, best move	Accuracy validation ~82%, thá»i gian dá»± Ä‘oÃ¡n ~1.2ms
4.3. Káº¿t quáº£ so sÃ¡nh
4.3.1. Káº¿t quáº£ Ä‘á»‹nh lÆ°á»£ng

RL: Ä‘áº¡t Ä‘iá»ƒm cao nháº¥t, Ä‘áº·c biá»‡t á»Ÿ cháº¿ Ä‘á»™ Medium vÃ  Hard.

Heuristic: á»•n Ä‘á»‹nh, nhÆ°ng kÃ©m RL á»Ÿ tá»‘c Ä‘á»™ cao.

DL: hiá»‡u quáº£ khÃ¡, gáº§n tÆ°Æ¡ng Ä‘Æ°Æ¡ng heuristic, tá»‘c Ä‘á»™ dá»± Ä‘oÃ¡n nhanh, phÃ¹ há»£p deploy thá»i gian thá»±c.

PhÆ°Æ¡ng phÃ¡p	Avg score	Lines cleared	Max height	Survival steps	Notes
Heuristic	3.450	180	14	210	Ráº¥t á»•n Ä‘á»‹nh
RL (DQN)	4.870	245	12	310	Chiáº¿n lÆ°á»£c tá»‘i Æ°u
DL	3.720	190	13	220	Tá»‘c Ä‘á»™ dá»± Ä‘oÃ¡n cao
4.3.2. Biá»ƒu Ä‘á»“ vÃ  trá»±c quan

ÄÆ°á»ng tÄƒng Ä‘iá»ƒm trung bÃ¬nh theo episode (RL).

So sÃ¡nh sá»‘ dÃ²ng xÃ³a giá»¯a 3 AI.

Äá»™ cao board theo thá»i gian.

(Trong bÃ¡o cÃ¡o chÃ­nh thá»©c, cÃ³ thá»ƒ chÃ¨n biá»ƒu Ä‘á»“ matplotlib tá»« quÃ¡ trÃ¬nh huáº¥n luyá»‡n.)

4.4. ÄÃ¡nh giÃ¡

Heuristic: tá»‘t cho baseline, nhanh nhÆ°ng háº¡n cháº¿ chiáº¿n lÆ°á»£c dÃ i háº¡n.

RL: Ä‘áº¡t Ä‘iá»ƒm tá»‘i Æ°u, há»c Ä‘Æ°á»£c combo dÃ i, cáº§n huáº¥n luyá»‡n lÃ¢u.

DL: tá»‘c Ä‘á»™ nhanh, thÃ­ch há»£p dá»± Ä‘oÃ¡n online, Ä‘á»™ chÃ­nh xÃ¡c hÆ¡i tháº¥p hÆ¡n RL.

Káº¿t luáº­n:

RL lÃ  phÆ°Æ¡ng phÃ¡p máº¡nh nháº¥t vá» Ä‘iá»ƒm sá»‘.

Heuristic vÃ  DL thÃ­ch há»£p cho deploy thá»i gian thá»±c.

CÃ³ thá»ƒ káº¿t há»£p: dÃ¹ng heuristic hoáº·c DL Ä‘á»ƒ Ä‘á» xuáº¥t hÃ nh Ä‘á»™ng â†’ RL refine â†’ tá»‘i Æ°u cáº£ tá»‘c Ä‘á»™ láº«n chiáº¿n lÆ°á»£c.